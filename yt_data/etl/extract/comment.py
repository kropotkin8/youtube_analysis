"""Extracci칩n de comentarios desde YouTube Data API v3 (commentThreads.list)."""
import logging
from typing import Optional

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

from yt_data.config import YT_API_KEY

logger = logging.getLogger(__name__)


def extract_comments(
    video_id: str,
    *,
    part: str = "snippet,replies",
    max_results: int = 100,
    order: str = "time",
    page_token: Optional[str] = None,
) -> tuple[list[dict], Optional[str]]:
    """
    Llama a commentThreads.list para un video y devuelve threads en bruto.

    Args:
        video_id: ID del video.
        part: Partes a solicitar.
        max_results: 1-100 por p치gina.
        order: "time" o "relevance".
        page_token: Token de paginaci칩n.

    Returns:
        (lista de items de commentThreads, next_page_token o None).
    """
    if not YT_API_KEY:
        raise ValueError("YT_API_KEY no configurada en .env")

    youtube = build("youtube", "v3", developerKey=YT_API_KEY)
    try:
        request = youtube.commentThreads().list(
            part=part,
            videoId=video_id,
            maxResults=min(max_results, 100),
            order=order,
            textFormat="plainText",
            pageToken=page_token or "",
        )
        response = request.execute()
        items = response.get("items", [])
        next_token = response.get("nextPageToken")
        logger.info("commentThreads.list video=%s: %d threads, next=%s", video_id, len(items), bool(next_token))
        return items, next_token
    except HttpError as e:
        logger.exception("commentThreads.list error: %s", e)
        raise


def extract_all_comments_for_video(
    video_id: str,
    *,
    max_pages: Optional[int] = None,
    max_results_per_page: int = 100,
) -> list[dict]:
    """
    Recorre todas las p치ginas de comentarios de un video y devuelve
    una lista plana de comentarios (top-level + replies aplanados).

    Returns:
        Lista de dicts, cada uno con keys: comment_id, video_id, author, text, published_at, like_count, parent_id, raw_snippet.
    """
    all_comments = []
    page_token = None
    pages = 0

    while True:
        items, next_token = extract_comments(
            video_id,
            max_results=max_results_per_page,
            page_token=page_token,
        )
        for thread in items:
            snippet = thread.get("snippet", {})
            top = snippet.get("topLevelComment", {})
            top_snippet = top.get("snippet", {})
            all_comments.append({
                "comment_id": top["id"],
                "video_id": video_id,
                "author": top_snippet.get("authorDisplayName"),
                "text": top_snippet.get("textDisplay") or top_snippet.get("textOriginal"),
                "published_at": top_snippet.get("publishedAt"),
                "like_count": int(top_snippet.get("likeCount", 0)),
                "parent_id": None,
                "raw_snippet": top_snippet,
            })
            reply_count = snippet.get("totalReplyCount", 0)
            if reply_count and "replies" in thread:
                for reply_comment in thread["replies"].get("comments", []):
                    rs = reply_comment.get("snippet", {})
                    all_comments.append({
                        "comment_id": reply_comment["id"],
                        "video_id": video_id,
                        "author": rs.get("authorDisplayName"),
                        "text": rs.get("textDisplay") or rs.get("textOriginal"),
                        "published_at": rs.get("publishedAt"),
                        "like_count": int(rs.get("likeCount", 0)),
                        "parent_id": rs.get("parentId"),
                        "raw_snippet": rs,
                    })
        pages += 1
        if not next_token or (max_pages and pages >= max_pages):
            break
        page_token = next_token

    return all_comments
